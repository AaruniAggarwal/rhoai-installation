apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    description: vLLM is a high-throughput and memory-efficient inference and serving
      engine for LLMs
    internal.config.kubernetes.io/previousKinds: Template
    internal.config.kubernetes.io/previousNames: vllm-runtime-template
    internal.config.kubernetes.io/previousNamespaces: opendatahub
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["single"]'
    openshift.io/display-name: vLLM ServingRuntime for KServe for Power
    openshift.io/provider-display-name: Red Hat, Inc.
    tags: rhods,rhoai,kserve,servingruntime
    template.openshift.io/documentation-url: https://github.com/opendatahub-io/vllm
    template.openshift.io/long-description: This template defines resources needed
      to deploy vLLM ServingRuntime with KServe in Red Hat OpenShift AI
  creationTimestamp: "2024-07-03T15:41:08Z"
  labels:
    app: odh-dashboard
    app.kubernetes.io/part-of: kserve
    app.opendatahub.io/kserve: "true"
    opendatahub.io/dashboard: "true"
    opendatahub.io/ootb: "true"
  name: vllm-runtime-template-ppc64le
  namespace: redhat-ods-applications
objects:
- apiVersion: serving.kserve.io/v1alpha1
  kind: ServingRuntime
  metadata:
    annotations:
      opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
      openshift.io/display-name: vLLM ServingRuntime for KServe for Power
    labels:
      opendatahub.io/dashboard: "true"
    name: vllm-runtime-ppc64le
  spec:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
    containers:
    - args:
      - --port=8080
      - --model=/mnt/models
      - --served-model-name={{.Name}}
      command:
      - /opt/conda/bin/python3
      - -m
      - vllm.entrypoints.openai.api_server
      env:
      - name: HF_HOME
        value: /tmp/hf_home
      image: PLACEHOLDER
      name: kserve-container
      ports:
      - containerPort: 8080
        protocol: TCP
    multiModel: false
    supportedModelFormats:
    - autoSelect: true
      name: vLLM
